import multiprocessing

import time
def wait():
    time.sleep(0.5)
    print("Done waiting")

process = multiprocessing.Process(target=wait)

# Add code here
process.start()
print("Finished")

process.join()

import time
def wait():
    time.sleep(0.5)
    print("Done waiting")

process = multiprocessing.Process(target=wait)

# Add code below
process.start()
process.join()
print("Finished")

import time
def wait():
    time.sleep(0.5)
    print("Done waiting")

# Add code below
start = time.time()

p1 = multiprocessing.Process(target=wait)
p2 = multiprocessing.Process(target=wait)

p1.start()
p2.start()

p1.join()
p2.join()

end = time.time()

elapsed = end - start

import time
def wait():
    time.sleep(0.5)
    print("Done waiting")

# Add code below
start = time.time()

processes = [multiprocessing.Process(target=wait) for _ in range(6)]

for process in processes:
    process.start()

for process in processes:
    process.join()

end = time.time()

elapsed = end - start

def sum3(x, y, z):
    print(x + y + z)

def list_average(values):
    print(sum(values) / len(values))

# Add code below
sum3_process = multiprocessing.Process(target=sum3, args=[3, 2, 5])
list_average_process = multiprocessing.Process(target=list_average, args=[[1, 2, 3, 4, 5]])

sum3_process.start()
list_average_process.start()

sum3_process.join()
list_average_process.join()

def sum3(x, y, z):
    print(x + y + z)
def sum3(x, y, z, shared_value):
    shared_value.value = x + y + z

float_value = multiprocessing.Value("f")
process = multiprocessing.Process(target=sum3, args=[5, 7, 4, float_value])
process.start()
process.join()

print(float_value.value)

def sum_values(first, last, shared_value):
    for i in range(first, last):
        shared_value.value += i

def sum_with_two_processes():
    N = 10000

    shared_value = multiprocessing.Value("i")
    process1 = multiprocessing.Process(target=sum_values, args=(1, N // 2, shared_value))
    process2 = multiprocessing.Process(target=sum_values, args=(N // 2, N, shared_value))

    process1.start()
    process2.start()

    process1.join()
    process2.join()
    return shared_value.value

# Add code below
results = []
for _ in range(10):
    result = sum_with_two_processes()
    results.append(result)

print(results)

def sum_values(first, last, shared_value):
    for i in range(first, last):
        with shared_value.get_lock():
            shared_value.value += i

def measure_runtime(function_to_measure):
    N = 10000
    shared_value = multiprocessing.Value("i")
    process1 = multiprocessing.Process(target=function_to_measure, args=(1, N // 2, shared_value))
    process2 = multiprocessing.Process(target=function_to_measure, args=(N // 2, N, shared_value))
    start = time.time()
    process1.start()
    process2.start()
    process1.join()
    process2.join()
    end = time.time()
    return end - start
    
# Add code below
def sum_values_improved(first, last, shared_value):
    value_sum = 0
    for i in range(first, last):
        value_sum += i
    with shared_value.get_lock():
        shared_value.value += value_sum

time_sum_values = measure_runtime(sum_values)
time_sum_values_improved = measure_runtime(sum_values_improved)

print(time_sum_values)
print(time_sum_values_improved)

import pandas as pd
job_postings = pd.read_csv("DataEngineer.csv")
num_rows = job_postings.shape[0]
num_cols = job_postings.shape[1]

import pandas as pd
job_postings = pd.read_csv("DataEngineer.csv")
num_rows = job_postings.shape[0]
num_cols = job_postings.shape[1]

job_postings["Job Description"] = job_postings["Job Description"].str.lower()
frequency = {}
frequency["postgres"] = job_postings["Job Description"].str.count("postgres").sum()
frequency["sql"] = job_postings["Job Description"].str.count("sql").sum()

print(frequency)

skills = pd.read_csv("Skills.csv")

frequency = {}
for skill_name in skills["Name"]:
    frequency[skill_name] = job_postings["Job Description"].str.count(skill_name).sum()
    
print(frequency["programming"])

import time

#frequency = {}
#for skill_name in skills["Name"]:
#    frequency[skill_name] = job_postings["Job Description"].str.count(skill_name).sum()
def count_skills(job_postings, skills):
    frequency = {}
    for skill_name in skills["Name"]:
        frequency[skill_name] = job_postings["Job Description"].str.count(skill_name).sum()
    return frequency


start = time.time()
count_skills(job_postings, skills)
end = time.time()
runtime = end - start

import math
def make_chunks(df, num_chunks):
    num_rows = df.shape[0]
    chunk_size = math.ceil(num_rows / num_chunks)
    return [df[i:i+chunk_size] for i in range(0, num_rows, chunk_size)]

skill_chunks = make_chunks(skills, 8)

import concurrent.futures

def increment(value):
    return value + 1

values = [1, 2, 3, 4, 5, 6, 7, 8]

# Add code here
with concurrent.futures.ProcessPoolExecutor() as executor:
    futures = [executor.submit(increment, value) for value in values]

results = [future.result() for future in futures]

import concurrent.futures
skill_chunks = make_chunks(skills, 8)

with concurrent.futures.ProcessPoolExecutor() as executor:
    futures = [executor.submit(count_skills, job_postings, skill_chunk) for skill_chunk in skill_chunks]

results = [future.result() for future in futures]
print(results[0])

merged_results = {}
for result in results:
    merged_results.update(result)

def count_skills(job_postings, skills):
    frequency = {}
    for skill_name in skills["Name"]:
        frequency[skill_name] = job_postings["Job Description"].str.count(skill_name).sum()
    return frequency

def count_skills_parallel(job_postings, skills, num_processes=4):
    # Calculate results using paralleld processing
    skill_chunks = make_chunks(skills, num_processes)
    with concurrent.futures.ProcessPoolExecutor() as executor:
        futures = [executor.submit(count_skills, job_postings, skill_chunk) for skill_chunk in skill_chunks]
    results = [future.result() for future in futures]
    # Merge results
    merged_results = {}
    for result in results:
        merged_results.update(result)
    return merged_results

import time

# Measure execution times here
start = time.time()
count_skills(job_postings, skills)
end = time.time()
time_normal = end - start

start = time.time()
count_skills_parallel(job_postings, skills)
end = time.time()
time_parallel = end - start

print(time_normal / time_parallel)

import math

def make_chunks(df, num_chunks):
    chunk_size = math.ceil(df.shape[0] / num_chunks)
    return [df[i:i+chunk_size] for i in range(0, df.shape[0], chunk_size)]
# Answer
def make_chunks(data, num_chunks):
    chunk_size = math.ceil(len(data) / num_chunks)
    return [data[i:i+chunk_size] for i in range(0, len(data), chunk_size)]

chunks = make_chunks([1, 2, 3, 4, 5, 6], 3)

import math

def make_chunks(df, num_chunks):
    chunk_size = math.ceil(df.shape[0] / num_chunks)
    return [df[i:i+chunk_size] for i in range(0, df.shape[0], chunk_size)]
# Answer
def make_chunks(data, num_chunks):
    chunk_size = math.ceil(len(data) / num_chunks)
    return [data[i:i+chunk_size] for i in range(0, len(data), chunk_size)]

chunks = make_chunks([1, 2, 3, 4, 5, 6], 3)

import concurrent.futures

def map_parallel(mapper, data, num_processes):
    chunks = make_chunks(data, num_processes)
    with concurrent.futures.ProcessPoolExecutor() as executor:
        futures = [executor.submit(mapper, chunk) for chunk in chunks]
    return [future.result() for future in futures]

values = [1, 4, 5, 2, 7, 21,     \
          31, 41, 3, 40, 5, 14,  \
          9, 32, 12, 18, 1, 30,  \
          6, 19, 23, 35, 12, 13, \
          0, 12, 42, 41, 11, 9]

# Write code here
results = map_parallel(max, values, 5)

values = [1, 4, 5, 2, 7, 21,     \
          31, 41, 3, 40, 5, 14,  \
          9, 32, 12, 18, 1, 30,  \
          6, 19, 23, 35, 12, 13, \
          0, 12, 42, 41, 11, 9]

chunks = make_chunks(values, 6)

# Write code here
from multiprocessing import Pool
pool = Pool(6)
results = pool.map(max, chunks)
pool.close()
pool.join()

values = [1, 4, 5, 2, 7, 21,     \
          31, 41, 3, 40, 5, 14,  \
          9, 32, 12, 18, 1, 30,  \
          6, 19, 23, 35, 12, 13, \
          0, 12, 42, 41, 11, 9]

chunks = make_chunks(values, 6)

# Write code here
with Pool(6) as pool:
    results = pool.map(max, chunks)

values = [1, 4, 5, 2, 7, 21,     \
          31, 41, 3, 40, 5, 14,  \
          9, 32, 12, 18, 1, 30,  \
          6, 19, 23, 35, 12, 13, \
          0, 12, 42, 41, 11, 9]

# Write code here
import functools
max_value = functools.reduce(max, values)

data = [1, 4, 5, 2, 7, 21,     \
        31, 41, 3, 40, 5, 14,  \
        9, 32, 12, 18, 1, 30,  \
        6, 19, 23, 35, 12, 13, \
        0, 12, 42, 41, 11, 9]

num_processes = 5

# Write code here
# Step 1: split
chunks = make_chunks(data, num_processes)

# Step 2: map
with Pool(num_processes) as pool:
    chunk_results = pool.map(max, chunks)

# Step 3: reduce
overall_result = functools.reduce(max, chunk_results)

data = [1, 4, 5, 2, 7, 21,     \
        31, 41, 3, 40, 5, 14,  \
        9, 32, 12, 18, 1, 30,  \
        6, 19, 23, 35, 12, 13, \
        0, 12, 43, 41, 11, 9]

def map_reduce(data, num_processes, mapper, reducer):
    chunks = make_chunks(data, num_processes)
    with Pool(num_processes) as pool:
        chunk_results = pool.map(mapper, chunks)
    return functools.reduce(reducer, chunk_results)

# Write code here
max_value = map_reduce(data, 4, max, max)

import pandas as pd
job_postings = pd.read_csv("DataEngineer.csv")
job_postings["Job Description"] = job_postings["Job Description"].str.lower()
skills = pd.read_csv("Skills.csv")

# Write code here
def mapper(skill_chunk):
    frequency = {}
    for skill_name in skill_chunk["Name"]:
        frequency[skill_name] = job_postings["Job Description"].str.count(skill_name).sum()
    return frequency

def reducer(freq_chunk1, freq_chunk2):
    freq_chunk1.update(freq_chunk2)
    return freq_chunk1
    
skill_freq = map_reduce(skills, 4, mapper, reducer)

import pandas as pd
job_postings = pd.read_csv("DataEngineer.csv")
job_postings["Job Description"] = job_postings["Job Description"].str.lower()
skills = pd.read_csv("Skills.csv")

# Write code here
def mapper(jobs_chunk):
    frequency = {}
    for skill_name in skills["Name"]:
        frequency[skill_name] = jobs_chunk["Job Description"].str.count(skill_name).sum()
    return frequency

def reducer(freq_chunk1, freq_chunk2):
    merged = {}
    for skill in freq_chunk1:
        merged[skill] = freq_chunk1[skill] + freq_chunk2[skill]
    return merged
    
skill_freq = map_reduce(job_postings, 4, mapper, reducer)

values = [98, 63, 55, 80, 45, 51, 91, 64, 65, 48, 48, 92, 76, 99, 57, 42, 79, 61, 63, 49]
min_value = map_reduce(values, 4, min, min)
print(min_value)

with open("english_words.txt") as f:
    words = [word.strip() for word in f.readlines()]
def map_max_length(words_chunk):
    return max([len(word) for word in words_chunk]) 

max_len = map_reduce(words, 4, map_max_length, max)
print(max_len)

with open("english_words.txt") as f:
    words = [word.strip() for word in f.readlines()]
def map_max_len_str(words_chunk):
    return max(words_chunk, key=len) 

def reduce_max_len_str(word1, word2):
    return map_max_len_str([word1, word2])


max_len_str = map_reduce(words, 4, map_max_len_str, reduce_max_len_str)
print(max_len_str)
